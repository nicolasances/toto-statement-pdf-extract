{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on PDF data extraction\n",
    "\n",
    "The goal of this notebook is to run experiments to create an algorithm that can: \n",
    "1. Extract data from DanskeBank quaterly statements into a structured form\n",
    "2. Compare that data with the user's expenses in Toto and create possible matches, for automated reconciliation (consolidation)\n",
    "\n",
    "## 1. Extraction of data from the kontoudskrift PDF\n",
    "\n",
    "The first step requires the extract of the data from the *Kontoudskrift* PDF statement received from DanskeBank.<br>\n",
    "After experimenting, I've created a utility class `KudExtract` that can do this with the method `process_pdf`.<br>\n",
    "That method will return a list of JSON objects (dicts) that contain a date, text (description) and amount (negative for expenses, positive for incomes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.KudExtract import KudExtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_kud(filepath, year, month, append_to_file=None): \n",
    "\n",
    "    # Starting from 2020.09 the kud switches to ',' as a decimal separator\n",
    "    dec_separator = '.'\n",
    "    thousands_separator = ','\n",
    "    if int(year) > 2020: \n",
    "        dec_separator = ','\n",
    "        thousands_separator = '.'\n",
    "    elif int(year) == 2020 and int(month) >= 9: \n",
    "        dec_separator = ','\n",
    "        thousands_separator = '.'\n",
    "\n",
    "    kud = KudExtract(year, decimal_separator=dec_separator, thousands_separator=thousands_separator)\n",
    "\n",
    "    kud_data = kud.process_pdf(filepath)\n",
    "\n",
    "    if not append_to_file == None: \n",
    "        # Save it to a file\n",
    "        with open(append_to_file, 'a') as file:\n",
    "            for item in kud_data: \n",
    "                file.write(f\"{item}\\n\")\n",
    "\n",
    "    return kud_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Kontoudskrift kuds/kud-2019.06.PDF for year 2019\n",
      "Extracted 758 lines\n",
      "Final lines retained: 136\n",
      "Processing Kontoudskrift kuds/kud-2021.03.PDF for year 2021\n",
      "Extracted 214 lines\n",
      "Final lines retained: 66\n",
      "Processing Kontoudskrift kuds/kud-2023.06.PDF for year 2023\n",
      "Extracted 1081 lines\n",
      "Final lines retained: 196\n",
      "Processing Kontoudskrift kuds/kud-2019.03.PDF for year 2019\n",
      "Extracted 522 lines\n",
      "Final lines retained: 120\n",
      "Processing Kontoudskrift kuds/kud-2021.12.PDF for year 2021\n",
      "Extracted 1019 lines\n",
      "Final lines retained: 136\n",
      "Processing Kontoudskrift kuds/kud-2023.03.PDF for year 2023\n",
      "Extracted 505 lines\n",
      "Final lines retained: 120\n",
      "Processing Kontoudskrift kuds/kud-2020.09.PDF for year 2020\n",
      "Extracted 474 lines\n",
      "Final lines retained: 104\n",
      "Processing Kontoudskrift kuds/kud-2022.09.PDF for year 2022\n",
      "Extracted 1046 lines\n",
      "Final lines retained: 178\n",
      "Processing Kontoudskrift kuds/kud-2018.09.PDF for year 2018\n",
      "Extracted 388 lines\n",
      "Final lines retained: 141\n",
      "Processing Kontoudskrift kuds/kud-2020.03.PDF for year 2020\n",
      "Extracted 697 lines\n",
      "Final lines retained: 119\n",
      "Processing Kontoudskrift kuds/kud-2022.06.PDF for year 2022\n",
      "Extracted 355 lines\n",
      "Final lines retained: 109\n",
      "Processing Kontoudskrift kuds/kud-2022.12.PDF for year 2022\n",
      "Extracted 989 lines\n",
      "Final lines retained: 195\n",
      "Processing Kontoudskrift kuds/kud-2018.06.PDF for year 2018\n",
      "Extracted 353 lines\n",
      "Final lines retained: 121\n",
      "Processing Kontoudskrift kuds/kud-2020.06.PDF for year 2020\n",
      "Extracted 327 lines\n",
      "Final lines retained: 58\n",
      "Processing Kontoudskrift kuds/kud-2020.12.PDF for year 2020\n",
      "Extracted 290 lines\n",
      "Final lines retained: 91\n",
      "Processing Kontoudskrift kuds/kud-2022.03.PDF for year 2022\n",
      "Extracted 1048 lines\n",
      "Final lines retained: 134\n",
      "Processing Kontoudskrift kuds/kud-2018.03.PDF for year 2018\n",
      "Extracted 147 lines\n",
      "Final lines retained: 41\n",
      "Processing Kontoudskrift kuds/kud-2021.09.PDF for year 2021\n",
      "Extracted 803 lines\n",
      "Final lines retained: 147\n",
      "Processing Kontoudskrift kuds/kud-2019.09.PDF for year 2019\n",
      "Extracted 710 lines\n",
      "Final lines retained: 107\n",
      "Processing Kontoudskrift kuds/kud-2023.09.PDF for year 2023\n",
      "Extracted 1251 lines\n",
      "Final lines retained: 191\n"
     ]
    }
   ],
   "source": [
    "dir = \"kuds/\"\n",
    "prefix = dir + \"kud-\"\n",
    "\n",
    "kud_data_filename = 'kud_data.txt'\n",
    "\n",
    "# Clear out previous files\n",
    "if os.path.exists(kud_data_filename): \n",
    "    os.remove(kud_data_filename)\n",
    "\n",
    "# Extract the data for every trimester\n",
    "kud_files_list = os.listdir(dir)\n",
    "\n",
    "for filename in kud_files_list: \n",
    "    \n",
    "    filepath = dir + filename;\n",
    "    \n",
    "    year = filepath[len(prefix):len(prefix) + 4]\n",
    "    month = filepath[-6:-4]\n",
    "    \n",
    "    kud_data = extract_data_from_kud(filepath, year, month, kud_data_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare and Predict\n",
    "\n",
    "In this section we experiment on how to compare and predict similarity between the recorded Toto expenses and the ones extracted from the DanskeBank statement. \n",
    "\n",
    "To do that we have the following logical steps: \n",
    "\n",
    "1. Generate Embeddings from the expenses descriptions both in Toto and Kud \n",
    "\n",
    "2. Create possible matches based on the amount.<br>\n",
    "Start by easy-matching expenses: \n",
    "    * When there's a 1:1 match between a Toto and a Kud expense, you have a match, no need to go further\n",
    "    * When there's a 1:many match, you need an algo that will help you choose which one is the actual match\n",
    "<br><br>\n",
    "\n",
    "3. In the 1:many situation, use an algorithm to compare embeddings (e.g. Cosine Similarity). <br>\n",
    "Concretely, that means comparing each KUD expense to each Toto Expense in the 1:many match and generating a similarity matrix. <br>\n",
    "That gives a **similarity score** ($S_j^{(i)}$) to the j-eth match in the i-eth 1:many match.\n",
    "\n",
    "4. In the 1:many situation, also do a similarity match between the dates.<br>\n",
    "You can approximate that by converting dates to YYYYMMDD integers and calculating the sqared difference: <br>\n",
    "$D_j^{(i)} = (D_k - D_t)^2$\n",
    "\n",
    "5. Train a model that takes $S$ and $D$ and generate a probability of match between a Kud expense and a Toto expense in a 1:many match.<br>\n",
    "To train that model you will first need to get a labeled dataset. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Generation of Embeddings\n",
    "\n",
    "The biggest problems in generating embeddings are: \n",
    "\n",
    "* **Language**. I'm using multi-lingual text and need to be able to generate comparable embeddings from text in different languages. I need a model with good cross-lingual transferability meaning that the learned sentence representations can capture semantic similarity and transfer knowledge across different languages. <br>\n",
    "There are models like LASER or BERT Multilingual that can do that, but they are **very heavy**, hence it is not feasible to store them in a running container as part of an API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"payments.json\", \"r\") as file: \n",
    "    toto_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "kud_data = []\n",
    "\n",
    "with open(kud_data_filename, \"r\") as file:\n",
    "    for line in file:\n",
    "        line = file.readline().replace('\\n', '')\n",
    "        kud_data.append(json.loads(line.replace('\\'', '\"')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenses_list = toto_data[\"expenses\"]\n",
    "\n",
    "# Only extract the list of expenses in the date range [10.2022, 12.2022]\n",
    "selected_toto_data = []\n",
    "for expense in expenses_list: \n",
    "    \n",
    "    date = str(expense[\"date\"])\n",
    "\n",
    "    if int(date[:6]) >= 201801 and int(date[:6]) <= 202309: \n",
    "        selected_toto_data.append({\n",
    "            \"date\": date[6:] + \".\" + date[4:6] + \".\" + date[:4], \n",
    "            \"text\": expense[\"description\"],\n",
    "            \"amount\": expense[\"amount\"],\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_series(training_example): \n",
    "\n",
    "    data = [\n",
    "        training_example[\"type\"],\n",
    "        training_example[\"kud\"][\"date\"],\n",
    "        training_example[\"kud\"][\"text\"],\n",
    "        training_example[\"kud\"][\"amount\"], \n",
    "        training_example[\"toto\"][\"date\"],\n",
    "        training_example[\"toto\"][\"text\"],\n",
    "        training_example[\"toto\"][\"amount\"], \n",
    "        training_example[\"label\"]\n",
    "    ]\n",
    "\n",
    "    return data\n",
    "\n",
    "def to_dataframe(training_examples): \n",
    "\n",
    "    data = []\n",
    "    \n",
    "    for ex in training_examples: \n",
    "        if ex[\"kud\"][\"text\"] == \"\": \n",
    "            continue\n",
    "        data.append(to_series(ex))\n",
    "\n",
    "    return pd.DataFrame(data, columns=[\"type\", \"kud_date\", \"kud_text\", \"kud_amount\", \"toto_date\", \"toto_text\", \"toto_amount\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 754\n",
      "Number of one-to-many: 401\n",
      "Will generate 754 negative examples\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "\n",
    "# Now let's try a reconciliation\n",
    "# List of reconciled expenses\n",
    "reconciled = []\n",
    "\n",
    "# List of potential matches\n",
    "one_to_many = []\n",
    "\n",
    "def month_of(date): \n",
    "    return date[3:5]\n",
    "\n",
    "num_onetomany = 0\n",
    "\n",
    "for kud_expense in kud_data: \n",
    "\n",
    "    amount_matches = []\n",
    "\n",
    "    for toto_expense in selected_toto_data: \n",
    "\n",
    "        if toto_expense[\"amount\"] == abs(kud_expense[\"amount\"]) and month_of(toto_expense[\"date\"]) == month_of(kud_expense[\"date\"]): \n",
    "            amount_matches.append(toto_expense)\n",
    "\n",
    "    if len(amount_matches) == 1: \n",
    "        reconciled.append({\"kud\": kud_expense, \"toto\": amount_matches[0]})\n",
    "    elif len(amount_matches) > 1: \n",
    "        one_to_many.append({\"kud\": kud_expense, \"toto\": amount_matches})\n",
    "        num_onetomany += len(amount_matches)\n",
    "\n",
    "\n",
    "print(f\"Number of matches: {len(reconciled)}\")\n",
    "print(f\"Number of one-to-many: {num_onetomany}\")\n",
    "\n",
    "# Generate some negative examples\n",
    "print(f\"Will generate {len(reconciled)} negative examples\")\n",
    "\n",
    "# The logic for negative examples is to take a reconciled i-th example and take the kud expense and i+1-th toto expense. They will most likely mismatch\n",
    "neg_examples = []\n",
    "for i in range(len(reconciled)): \n",
    "\n",
    "    j = i+1\n",
    "\n",
    "    if j == len(reconciled): \n",
    "        j = 0\n",
    "    \n",
    "    neg_example = {\"kud\": reconciled[i][\"kud\"], \"toto\": reconciled[j][\"toto\"]}\n",
    "\n",
    "    neg_examples.append(neg_example)\n",
    "\n",
    "training_examples = []\n",
    "\n",
    "for item in one_to_many: \n",
    "    for toto_expense in item[\"toto\"]: \n",
    "        training_example = {\"type\": \"onetomany\", \"kud\": item[\"kud\"], \"toto\": toto_expense, \"label\": \"?\"}\n",
    "        training_examples.append(training_example)\n",
    "\n",
    "for item in reconciled: \n",
    "    training_example = {\"type\": \"reconciled\", \"kud\": item[\"kud\"], \"toto\": item[\"toto\"], \"label\": 1}\n",
    "    training_examples.append(training_example)\n",
    "\n",
    "for item in neg_examples: \n",
    "    training_example = {\"type\": \"negative\", \"kud\": item[\"kud\"], \"toto\": item[\"toto\"], \"label\": 0}\n",
    "    training_examples.append(training_example)\n",
    "\n",
    "df = to_dataframe(training_examples=training_examples)\n",
    "df.to_csv(\"training.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
